{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_NAME = \"lenet_mnist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 获取当前 notebook 的路径\n",
    "notebook_path = os.getcwd()\n",
    "parent_path = os.path.dirname(notebook_path)  # 获取上一级目录路径\n",
    "\n",
    "# 添加到 sys.path\n",
    "sys.path.append(parent_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from src.models.LeNet import LeNet\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 MNIST 数据集\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(0.1307, 0.3081)])\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 选择设备（GPU or CPU）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED = \"./\" + TASK_NAME + \"/pretrained_model.pth\"\n",
    "if not os.path.exists(\"./\" + TASK_NAME):\n",
    "    os.makedirs(\"./\" + TASK_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建第一个检查点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_cal(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Accuracy 5.69%, Loss: 0.0012\n",
      "Epoch 1, Accuracy 42.68%, Loss: 0.1223\n",
      "Epoch 1, Accuracy 55.66%, Loss: 0.2367\n",
      "Epoch 1, Accuracy 57.53%, Loss: 0.3363\n",
      "Epoch 1, Accuracy 63.10%, Loss: 0.4152\n",
      "Epoch 1, Accuracy 73.21%, Loss: 0.4794\n",
      "Epoch 1, Accuracy 77.64%, Loss: 0.5332\n",
      "Accuracy reached 75%, saving model and stopping training.\n"
     ]
    }
   ],
   "source": [
    "# 初始化模型\n",
    "model = LeNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()  # 交叉熵损失\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adam 优化器\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Adam 优化器\n",
    "\n",
    "accuracy_threshold = 75\n",
    "# 训练模型\n",
    "num_epochs = 10\n",
    "\n",
    "stop_training = False\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for iter, data in enumerate(trainloader):\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "        if iter % 100 == 0:\n",
    "            accuracy = accuracy_cal(model) \n",
    "            model.train()\n",
    "            print(f\"Epoch {epoch+1}, Accuracy {accuracy:.2f}%, Loss: {running_loss / len(trainloader):.4f}\")\n",
    "            if accuracy >= accuracy_threshold:\n",
    "                stop_training = True\n",
    "                print(f\"Accuracy reached {accuracy_threshold}%, saving model and stopping training.\")\n",
    "                torch.save(model.state_dict(), PRE_TRAINED)  # 保存模型\n",
    "                break\n",
    "    if stop_training:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用delta-lora、lc-checkpoint、lc+delta与baseline对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, images, labels):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # print(\"LoRA+LC Training Loss (Decomposed): {}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECOMPOSED_LAYERS = ['classifier.1.weight', 'classifier.3.weight']\n",
    "RANK = -1\n",
    "SCALING = -1\n",
    "\n",
    "DLORA_LC_LOC = \"./\" + TASK_NAME + \"/dlora-lc\"\n",
    "if not os.path.exists(DLORA_LC_LOC):\n",
    "    os.makedirs(DLORA_LC_LOC)\n",
    "\n",
    "LC_LOC = \"./\" + TASK_NAME + \"./lc\"\n",
    "if not os.path.exists(LC_LOC):\n",
    "    os.makedirs(LC_LOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SunJingYi\\AppData\\Local\\Temp\\ipykernel_36128\\4081562243.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  baseline_model.load_state_dict(torch.load(PRE_TRAINED))\n",
      "C:\\Users\\SunJingYi\\AppData\\Local\\Temp\\ipykernel_36128\\4081562243.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  lc_checkpoint_model.load_state_dict(torch.load(PRE_TRAINED))\n",
      "C:\\Users\\SunJingYi\\AppData\\Local\\Temp\\ipykernel_36128\\4081562243.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  load_sd_decomp(torch.load(PRE_TRAINED), delta_lora_model, DECOMPOSED_LAYERS)\n",
      "C:\\Users\\SunJingYi\\AppData\\Local\\Temp\\ipykernel_36128\\4081562243.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  load_sd_decomp(torch.load(PRE_TRAINED), dlora_lc_model, DECOMPOSED_LAYERS)\n",
      "C:\\Users\\SunJingYi\\AppData\\Local\\Temp\\ipykernel_36128\\4081562243.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  last_dlora_lc_baseline_checkpoint.load_state_dict(torch.load(PRE_TRAINED))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full accuracy: 80.05, LC accuracy: 80.05, Decomposed-Full accuracy: 78.53, Decomposed-Restored accuracy: 78.53\n",
      "Full accuracy: 81.97, LC accuracy: 81.98, Decomposed-Full accuracy: 79.04, Decomposed-Restored accuracy: 79.04\n",
      "Full accuracy: 83.8, LC accuracy: 83.79, Decomposed-Full accuracy: 79.31, Decomposed-Restored accuracy: 79.31\n",
      "Full accuracy: 84.88, LC accuracy: 84.89, Decomposed-Full accuracy: 79.65, Decomposed-Restored accuracy: 79.65\n",
      "Full accuracy: 86.43, LC accuracy: 86.42, Decomposed-Full accuracy: 79.83, Decomposed-Restored accuracy: 79.83\n",
      "Full accuracy: 86.97, LC accuracy: 86.98, Decomposed-Full accuracy: 80.21, Decomposed-Restored accuracy: 80.21\n",
      "Full accuracy: 87.67, LC accuracy: 87.67, Decomposed-Full accuracy: 80.48, Decomposed-Restored accuracy: 80.48\n",
      "Full accuracy: 88.1, LC accuracy: 88.11, Decomposed-Full accuracy: 80.58, Decomposed-Restored accuracy: 80.58\n",
      "Full accuracy: 88.98, LC accuracy: 88.98, Decomposed-Full accuracy: 80.84, Decomposed-Restored accuracy: 80.84\n",
      "Full accuracy: 89.34, LC accuracy: 89.35, Decomposed-Full accuracy: 81.06, Decomposed-Restored accuracy: 81.06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 195\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# if iter % 100 == 0:\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m#     accuracy = accuracy_cal(dlora_lc_model) \u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m#     dlora_lc_model.train()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m#     print(\"Full accuracy: {}, LC accuracy: {}, Decomposed-Full accuracy: {}, Decomposed-Restored accuracy: {}\".format(\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m#         full_accuracy, lc_accuracy, decomposed_full_accuracy, restored_accuracy))\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 195\u001b[0m     full_accuracy\u001b[38;5;241m.\u001b[39mappend(\u001b[43maccuracy_cal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbaseline_model\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    196\u001b[0m     decomposed_full_accuracy\u001b[38;5;241m.\u001b[39mappend(accuracy_cal(dlora_lc_model))\n\u001b[0;32m    197\u001b[0m     restored_model \u001b[38;5;241m=\u001b[39m lazy_restore(dlora_lc_weights, dlora_lc_decomp_weights, bias, LeNet(), \n\u001b[0;32m    198\u001b[0m                                   last_dlora_lc_baseline_checkpoint\u001b[38;5;241m.\u001b[39mstate_dict(), DECOMPOSED_LAYERS, \n\u001b[0;32m    199\u001b[0m                                   rank \u001b[38;5;241m=\u001b[39m RANK, scaling \u001b[38;5;241m=\u001b[39m SCALING)\n",
      "Cell \u001b[1;32mIn[25], line 6\u001b[0m, in \u001b[0;36maccuracy_cal\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m      4\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m testloader:\n\u001b[0;32m      7\u001b[0m         images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(images)\n",
      "File \u001b[1;32mc:\\Users\\SunJingYi\\.conda\\envs\\py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\SunJingYi\\.conda\\envs\\py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\SunJingYi\\.conda\\envs\\py310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\SunJingYi\\.conda\\envs\\py310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\SunJingYi\\.conda\\envs\\py310\\lib\\site-packages\\torchvision\\datasets\\mnist.py:146\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\SunJingYi\\.conda\\envs\\py310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\SunJingYi\\.conda\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\SunJingYi\\.conda\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\SunJingYi\\.conda\\envs\\py310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SunJingYi\\.conda\\envs\\py310\\lib\\site-packages\\torchvision\\transforms\\functional.py:350\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SunJingYi\\.conda\\envs\\py310\\lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:917\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    912\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    913\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected tensor to be a tensor image of size (..., C, H, W). Got tensor.size() = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m     )\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n\u001b[1;32m--> 917\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    919\u001b[0m dtype \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    920\u001b[0m mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(mean, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.models.LeNet_LowRank import getBase, LeNet_LowRank, load_sd_decomp\n",
    "import src.main as lc\n",
    "from src.utils.utils import evaluate_accuracy, lazy_restore, evaluate_compression\n",
    "import old_lc.main as olc\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 1\n",
    "\n",
    "full_accuracy = []\n",
    "decomposed_full_accuracy = []\n",
    "restored_accuracy = []\n",
    "lc_accuracy = []\n",
    "\n",
    "# 不使用lora\n",
    "\n",
    "baseline_model = LeNet().to(device)\n",
    "lc_checkpoint_model = LeNet().to(device)\n",
    "\n",
    "baseline_model.load_state_dict(torch.load(PRE_TRAINED))\n",
    "lc_checkpoint_model.load_state_dict(torch.load(PRE_TRAINED))\n",
    "\n",
    "# 使用lora\n",
    "\n",
    "w, b = getBase(baseline_model)\n",
    "delta_lora_model = LeNet_LowRank(w, b, rank = RANK).to(device)\n",
    "dlora_lc_model = LeNet_LowRank(w, b, rank = RANK).to(device)\n",
    "\n",
    "load_sd_decomp(torch.load(PRE_TRAINED), delta_lora_model, DECOMPOSED_LAYERS)\n",
    "load_sd_decomp(torch.load(PRE_TRAINED), dlora_lc_model, DECOMPOSED_LAYERS)\n",
    "\n",
    "# 对应的优化器\n",
    "learning_rate = 0.01\n",
    "baseline_optimizer = torch.optim.SGD(baseline_model.parameters(), lr = learning_rate)\n",
    "lc_checkpoint_optimizer = torch.optim.SGD(lc_checkpoint_model.parameters(), lr = learning_rate)\n",
    "delta_lora_optimizer = torch.optim.SGD(delta_lora_model.parameters(), lr = learning_rate)\n",
    "dlora_lc_optimizer = torch.optim.SGD(dlora_lc_model.parameters(), lr = learning_rate)\n",
    "\n",
    "# delta-lc压缩，创建第一个压缩点\n",
    "current_set = 0\n",
    "current_iter = 0\n",
    "set_path = \"/set_{}\".format(current_set)\n",
    "if not os.path.exists(DLORA_LC_LOC + set_path):\n",
    "    os.makedirs(DLORA_LC_LOC + set_path)\n",
    "dlora_lc_model = dlora_lc_model.to('cpu')\n",
    "dlora_lc_weights, dlora_lc_decomp_weights = lc.extract_weights(dlora_lc_model, DLORA_LC_LOC + \"/set_{}\".format(current_set), DECOMPOSED_LAYERS)\n",
    "dlora_lc_model = dlora_lc_model.to(device)\n",
    "# 上一个基线检查点，用于模拟恢复\n",
    "last_dlora_lc_baseline_checkpoint = LeNet()\n",
    "last_dlora_lc_baseline_checkpoint.load_state_dict(torch.load(PRE_TRAINED))\n",
    "\n",
    "\n",
    "# lc 压缩\n",
    "current_iter_old_lc = 0\n",
    "current_set_old_lc = 0\n",
    "\n",
    "lc_checkpoint_model = lc_checkpoint_model.to('cpu')\n",
    "cstate = lc_checkpoint_model.state_dict()\n",
    "set_path = \"/set_{}\".format(current_set_old_lc)\n",
    "if not os.path.exists(LC_LOC + set_path):\n",
    "    os.makedirs(LC_LOC + set_path)\n",
    "prev_state = olc.extract_weights(cstate, LC_LOC + set_path, DECOMPOSED_LAYERS)\n",
    "lc_checkpoint_model = lc_checkpoint_model.to(device)\n",
    "\n",
    "# 训练\n",
    "for epoch in range(num_epochs):\n",
    "    for iter, data in enumerate(trainloader):\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "        # baseline训练\n",
    "        train_model(baseline_model, baseline_optimizer, images, labels)\n",
    "\n",
    "        # lc-checkpoint训练\n",
    "        train_model(lc_checkpoint_model, lc_checkpoint_optimizer, images, labels)\n",
    "\n",
    "        # delta-lora训练\n",
    "        # train_model(delta_lora_model, delta_lora_optimizer, images, labels)\n",
    "\n",
    "        # delta-lc训练\n",
    "        train_model(dlora_lc_model, dlora_lc_optimizer, images, labels)\n",
    "    \n",
    "        ########################################\n",
    "        # dlora-lc部分\n",
    "        ########################################\n",
    "        # 每10个iteration保存新的基线检查点\n",
    "        if iter == 0 and epoch == 0:\n",
    "            pass\n",
    "        elif iter % 10 == 0:\n",
    "            # 模拟恢复：\n",
    "            # 1.读取最近的基线检查点，这里持久保存最近的基线检查点，因此可以减少一个读取过程\n",
    "            # 2.基线检查点加一堆delta lora来恢复到最新的完整检查点\n",
    "            last_dlora_lc_baseline_checkpoint = lazy_restore(dlora_lc_weights, dlora_lc_decomp_weights, bias, LeNet(), \n",
    "                                                    last_dlora_lc_baseline_checkpoint.state_dict(), DECOMPOSED_LAYERS, rank = RANK, scaling = SCALING)\n",
    "            \n",
    "            current_set += 1\n",
    "            current_iter = 0\n",
    "\n",
    "            # 3.保存最新的检查点\n",
    "            set_path = \"/set_{}\".format(current_set)\n",
    "            if not os.path.exists(DLORA_LC_LOC + set_path):\n",
    "                os.makedirs(DLORA_LC_LOC + set_path)\n",
    "            \n",
    "            # Rebuilding LoRA layers => reset model!\n",
    "            w, b = getBase(last_dlora_lc_baseline_checkpoint)\n",
    "            dlora_lc_model = LeNet_LowRank(w, b, rank = RANK)\n",
    "            dlora_lc_optimizer = torch.optim.SGD(dlora_lc_model.parameters(), lr = learning_rate)\n",
    "            load_sd_decomp(last_dlora_lc_baseline_checkpoint.state_dict(), dlora_lc_model, DECOMPOSED_LAYERS)\n",
    "            \n",
    "            dlora_lc_model = dlora_lc_model.to('cpu')\n",
    "            dlora_lc_weights, dlora_lc_decomp_weights = lc.extract_weights(dlora_lc_model, DLORA_LC_LOC + \n",
    "                                                    \"/set_{}\".format(current_set), DECOMPOSED_LAYERS)\n",
    "            dlora_lc_model = dlora_lc_model.to(device)\n",
    "        else:\n",
    "            # delta lora\n",
    "            dlora_lc_model = dlora_lc_model.to('cpu')\n",
    "            delta, decomp_delta, bias = lc.generate_delta(dlora_lc_weights, dlora_lc_decomp_weights, dlora_lc_model.state_dict(), DECOMPOSED_LAYERS)\n",
    "            dlora_lc_model = dlora_lc_model.to(device)\n",
    "            # lc checkpoint compression\n",
    "            compressed_delta, full_delta, compressed_dcomp_delta, full_dcomp_delta  = lc.compress_delta(delta, decomp_delta)\n",
    "            # save\n",
    "            lc.save_checkpoint(compressed_delta, compressed_dcomp_delta, bias, current_iter, DLORA_LC_LOC + \"/set_{}\".format(current_set))\n",
    "            # update weights\n",
    "            dlora_lc_weights = np.add(dlora_lc_weights, full_delta) # Replace base with latest for delta to accumulate.\n",
    "            dlora_lc_decomp_weights = np.add(dlora_lc_decomp_weights, full_dcomp_delta)\n",
    "\n",
    "            current_iter += 1\n",
    "\n",
    "        ########################################\n",
    "        # lc部分\n",
    "        ########################################\n",
    "        if iter == 0 and epoch == 0:\n",
    "            pass\n",
    "        else:\n",
    "            if iter % 10 == 0:\n",
    "                lc_checkpoint_model = lc_checkpoint_model.to('cpu')\n",
    "                cstate = lc_checkpoint_model.state_dict()\n",
    "                current_set_old_lc += 1\n",
    "                current_iter_old_lc = 0\n",
    "                set_path = \"/set_{}\".format(current_set_old_lc)\n",
    "                if not os.path.exists(LC_LOC + set_path):\n",
    "                    os.makedirs(LC_LOC + set_path)\n",
    "                # torch.save(cstate, SAVE_LOC_OLC + set_path + \"/initial_model.pt\")\n",
    "                prev_state = olc.extract_weights(cstate, LC_LOC + set_path, DECOMPOSED_LAYERS)\n",
    "                lc_checkpoint_model = lc_checkpoint_model.to(device)\n",
    "            else:\n",
    "                lc_checkpoint_model = lc_checkpoint_model.to('cpu')\n",
    "                cstate = lc_checkpoint_model.state_dict()\n",
    "                old_lc_delta, old_lc_bias = olc.generate_delta(prev_state, cstate, DECOMPOSED_LAYERS)\n",
    "                # print(\"Compressing delta for old_lc\")\n",
    "                # compressed_delta = olc.compress_delta(old_lc_delta, num_bits = 3)\n",
    "                olc_compressed_delta, update_prev = olc.compress_data(old_lc_delta, num_bits = 3)\n",
    "                olc.save_checkpoint(LC_LOC + \"/set_{}\".format(current_set_old_lc), olc_compressed_delta, \n",
    "                                    old_lc_bias, current_iter_old_lc)\n",
    "                prev_state = np.add(prev_state, update_prev)\n",
    "                current_iter_old_lc += 1\n",
    "                lc_checkpoint_model = lc_checkpoint_model.to(device)\n",
    "\n",
    "        # if iter % 100 == 0:\n",
    "        #     accuracy = accuracy_cal(dlora_lc_model) \n",
    "        #     dlora_lc_model.train()\n",
    "\n",
    "        #     baseline_acc = accuracy_cal(baseline_model)\n",
    "        #     baseline_model.train()\n",
    "\n",
    "        #     restored_model = lazy_restore(dlora_lc_weights, base_decomp, bias, LeNet(), \n",
    "        #                                   original.state_dict(), DECOMPOSED_LAYERS, \n",
    "        #                                   rank = RANK, scaling = SCALING)\n",
    "        #     restored_lc_model = LeNet().to(device)\n",
    "        #     restored_lc_model.load_state_dict(olc.restore_state_dict(prev_state, old_lc_bias, \n",
    "        #                                                           restored_model.state_dict(), DECOMPOSED_LAYERS))\n",
    "            \n",
    "        #     lc_accuracy = accuracy_cal(lc_checkpoint_model)\n",
    "\n",
    "        #     print(f\"Epoch {epoch+1}, DLORA LC Accuracy {accuracy:.2f}%, baseline acc {baseline_acc:.2f}%\")\n",
    "    \n",
    "        # if iter % 100 == 0 and iter != 0:\n",
    "        #     full_accuracy = accuracy_cal(baseline_model)\n",
    "        #     decomposed_full_accuracy = accuracy_cal(dlora_lc_model)\n",
    "        #     restored_model = lazy_restore(dlora_lc_weights, dlora_lc_decomp_weights, bias, LeNet(), \n",
    "        #                                   last_dlora_lc_baseline_checkpoint.state_dict(), DECOMPOSED_LAYERS, \n",
    "        #                                   rank = RANK, scaling = SCALING)\n",
    "        #     restored_model = restored_model.to(device)\n",
    "        #     restored_accuracy = accuracy_cal(restored_model)\n",
    "        #     restored_model = restored_model.to('cpu')\n",
    "        #     restored_lc_model = LeNet()\n",
    "        #     restored_lc_model.load_state_dict(olc.restore_state_dict(prev_state, old_lc_bias, \n",
    "        #                                                           restored_model.state_dict(), DECOMPOSED_LAYERS))\n",
    "        #     restored_lc_model = restored_lc_model.to(device)\n",
    "        #     lc_accuracy = accuracy_cal(restored_lc_model)\n",
    "        #     restored_lc_model = restored_lc_model.to('cpu')\n",
    "        #     print(\"Full accuracy: {}, LC accuracy: {}, Decomposed-Full accuracy: {}, Decomposed-Restored accuracy: {}\".format(\n",
    "        #         full_accuracy, lc_accuracy, decomposed_full_accuracy, restored_accuracy))\n",
    "\n",
    "        if iter % 100 == 0 and iter != 0:\n",
    "            full_accuracy.append(accuracy_cal(baseline_model))\n",
    "            decomposed_full_accuracy.append(accuracy_cal(dlora_lc_model))\n",
    "            restored_model = lazy_restore(dlora_lc_weights, dlora_lc_decomp_weights, bias, LeNet(), \n",
    "                                          last_dlora_lc_baseline_checkpoint.state_dict(), DECOMPOSED_LAYERS, \n",
    "                                          rank = RANK, scaling = SCALING)\n",
    "            \n",
    "            restored_model = restored_model.to(device)\n",
    "            restored_accuracy.append(accuracy_cal(restored_model))\n",
    "            restored_model = restored_model.to('cpu')\n",
    "            restored_lc_model = LeNet()\n",
    "            restored_lc_model.load_state_dict(olc.restore_state_dict(prev_state, old_lc_bias, \n",
    "                                                                  restored_model.state_dict(), DECOMPOSED_LAYERS))\n",
    "            restored_lc_model = restored_lc_model.to(device)\n",
    "            lc_accuracy.append(accuracy_cal(restored_lc_model))\n",
    "            restored_lc_model = restored_lc_model.to('cpu')\n",
    "            print(\"Full accuracy: {}, LC accuracy: {}, Decomposed-Full accuracy: {}, Decomposed-Restored accuracy: {}\".format(\n",
    "                full_accuracy[-1], lc_accuracy[-1], decomposed_full_accuracy[-1], restored_accuracy[-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (30, 5))\n",
    "plt.title(\"LeNet, Accuracy\")\n",
    "plt.plot(full_accuracy, label = \"Default LeNet\")\n",
    "plt.plot(lc_accuracy, label = \"LC LeNet\")\n",
    "plt.plot(decomposed_full_accuracy, label = \"dLoRA LeNet\")\n",
    "plt.plot(restored_accuracy, label = \"dLoRA + LC LeNet\")\n",
    "plt.xticks([x for x in range(0, 120) if x % 6 == 0], [x for x in range(0, 20)])\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "plt.savefig(\"./\" + TASK_NAME + \"/lenet_accuracy.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()  # 关闭图像，释放资源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rangex = [x for x in range(0, 120) if x % 6 == 0]\n",
    "rangey = [x for x in range(0, 20)]\n",
    "plt.figure(figsize = (40, 10))\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "ax1.set_title(\"LeNet Absolute Accuracy Loss (Default LeNet vs LC + dLoRA LeNet / LC LeNet)\")\n",
    "plt.plot(np.abs(np.subtract(np.array(full_accuracy), \n",
    "                     np.array(restored_accuracy))), label = \"LC + dLoRA LeNet\")\n",
    "plt.plot(np.abs(np.subtract(np.array(full_accuracy), \n",
    "                     np.array(lc_accuracy))), label = \"LC LeNet\")\n",
    "plt.legend()\n",
    "plt.xticks(rangex, rangey)\n",
    "plt.ylabel(\"Absolute Accuracy Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.axhline(y = 0.05, color = 'r')\n",
    "plt.ylim(0, 0.5)\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "ax2.set_title(\"LeNet Absolute Restoration Accuracy Loss (LC + dLoRA LeNet & LC LeNet)\")\n",
    "plt.plot(np.abs(np.subtract(np.array(restored_accuracy), \n",
    "                     np.array(decomposed_full_accuracy))), label = \"LC + dLoRA LeNet\")\n",
    "plt.plot(np.abs(np.subtract(np.array(full_accuracy), \n",
    "                     np.array(lc_accuracy))), label = \"LC LeNet\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 0.5)\n",
    "plt.axhline(y = 0.05, color = 'r')\n",
    "plt.xticks(rangex, rangey)\n",
    "plt.ylabel(\"Absolute Accuracy Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "# plt.show()\n",
    "\n",
    "plt.savefig(\"./\" + TASK_NAME + \"/lenet_loss.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()  # 关闭图像，释放资源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def getsize(sl):\n",
    "    dir = [x for x in os.listdir(sl)]\n",
    "    csize, usize = 0, 0\n",
    "    for set in dir:\n",
    "        for f in os.listdir(sl + \"/\" + set):\n",
    "            fp = sl + \"/{}/{}\".format(set, f)\n",
    "            csize += os.path.getsize(fp)\n",
    "            usize += 250 * math.pow(2, 10) # torch checkpoint same size\n",
    "    return csize, usize,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LC-Checkpoint + GZIP\n",
      "Compression Ratio: 918.528%, Space Savings: 89.113%\n",
      "LoRA + LC-Checkpoint + GZIP\n",
      "Compression Ratio: 2558.901%, Space Savings: 96.092%\n"
     ]
    }
   ],
   "source": [
    "compressed_size, uncompressed_size = getsize(DLORA_LC_LOC)\n",
    "a, b = evaluate_compression(uncompressed_size, compressed_size)\n",
    "compressed_size, uncompressed_size = getsize(LC_LOC)\n",
    "a1, b1 = evaluate_compression(uncompressed_size, compressed_size)\n",
    "\n",
    "print(\"LC-Checkpoint + GZIP\")\n",
    "print(\"Compression Ratio: {}%, Space Savings: {}%\".format(a1, b1))\n",
    "print(\"LoRA + LC-Checkpoint + GZIP\")\n",
    "print(\"Compression Ratio: {}%, Space Savings: {}%\".format(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = {\n",
    "    \"full_acc\" : full_accuracy,\n",
    "    \"decomposed_restored_accuracy\" : restored_accuracy,\n",
    "    \"decomposed_full_accuracy\" : decomposed_full_accuracy,\n",
    "    \"lc_restored_accuracy\" : lc_accuracy\n",
    "}\n",
    "with open(\"./\" + TASK_NAME + \"/data.json\", 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
